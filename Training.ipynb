{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of DNNs with different weights, predictor sets, and cost functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from torch.optim import lr_scheduler\n",
    "import pytorchltr.loss\n",
    "import time\n",
    "\n",
    "# Import custom functions\n",
    "import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory\n",
    "%cd '/path/to/working_directory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input data file\n",
    "data_file = 'Sample_dat2.csv'\n",
    "\n",
    "resp = 'Label'\n",
    "mtype = ['full','weighted','ndcg']\n",
    "\n",
    "# Possibilities are\n",
    "# mtype[0]: 'full', 'eco' -> predictor sets\n",
    "# mtype[1]: 'weighted', 'unweighted' -> observation weights\n",
    "#                                       Note that option 'unweighted' expects a 'weighted' model object (output) from \n",
    "#                                       which training is continued\n",
    "# mtype[3]: 'cel', 'ndcg' -> cost functions\n",
    "\n",
    "# Define Features\n",
    "if mtype[0] == 'full':\n",
    "    feana=['TRI','TAVE_Summer','PREC_Winter','PREC_Summer','FCF',\n",
    "       'SoilR','SoilF',\n",
    "       'ForestQ95','NDVI','NDVI_SD',\n",
    "       \"Doy_sin\",\"Doy_cos\"]\n",
    "    \n",
    "elif mtype[0] == 'eco':\n",
    "    feana=['TRI','TAVE_Summer','PREC_Winter','PREC_Summer','FCF',\n",
    "           'SoilR','SoilF',\n",
    "           'ForestQ95','NDVI','NDVI_SD']\n",
    "    \n",
    "# Define Output name\n",
    "whole_mod='Model_' + mtype[0] + '_' + mtype[1] + '_' + mtype[2] + '.pth'\n",
    "\n",
    "## Parameter dictionnary for Fitting    \n",
    "# Main\n",
    "params = {}\n",
    "\n",
    "params['num_workers'] = 17\n",
    "params['lr'] = {}\n",
    "params['device'] = 'cuda' # shoud be 'cpu' if no CUDA-ready GPU is available.\n",
    "params['num_filts'] = 380\n",
    "params['batch_size'] = 250\n",
    "\n",
    "   \n",
    "if mtype[1] == 'unweighted':\n",
    "    params['num_epochs'] = 100\n",
    "else:\n",
    "    params['num_epochs'] = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, data_file, label, phase, feats):       \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_file (string): Path to the csv file with annotations.\n",
    "            label: which column should be selected as label\n",
    "            phase: training or validation data?\n",
    "        \"\"\"\n",
    "        full_frame = pd.read_csv(data_file,low_memory=False)\n",
    "\n",
    "        meta_frame = full_frame[full_frame['Set'] == phase].copy()\n",
    "\n",
    "        lisbet=list(range(0,len(meta_frame)))\n",
    "        meta_frame.index=lisbet         \n",
    "        \n",
    "        feat_frame = meta_frame[feats]\n",
    "\n",
    "        self.feat_frame = feat_frame\n",
    "        self.labels = meta_frame[label].astype(int)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.feat_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        labels = np.array(self.labels.loc[idx])\n",
    "        \n",
    "        feats = np.array(self.feat_frame.loc[idx,:])\n",
    "\n",
    "        return feats, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "dase_train = FullDataset(data_file = data_file, \n",
    "                         label = resp, \n",
    "                         phase ='train',\n",
    "                         feats = feana)\n",
    "\n",
    "# Test\n",
    "dase_test = FullDataset(data_file = data_file, \n",
    "                        label = resp, \n",
    "                        phase = 'test',\n",
    "                        feats = feana)\n",
    "\n",
    "# Define dataset sizes to calculate loss\n",
    "dataset_sizes = {'train': len(dase_train), 'test': len(dase_test)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define balanced sampling for the DataLoader used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out label vector from training data\n",
    "target = dase_train.labels.to_numpy()\n",
    "\n",
    "# Create dictionnary with sampling weights for each label (inversely proportional to frequency)\n",
    "cls=np.array(torch.unique(torch.tensor(target))).astype(int)\n",
    "class_sample_count = np.array(torch.unique(torch.tensor(target), return_counts = True)[1])\n",
    "weight = 1. / class_sample_count\n",
    "weidict=dict(zip(cls,weight))\n",
    "\n",
    "# Create WeightedRandomSampler\n",
    "samples_weight = torch.from_numpy(np.array([weidict[t] for t in target])).double()\n",
    "\n",
    "# Add definitions\n",
    "params['sampler'] = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "params['num_classes'] = len(set(target))\n",
    "params['freqs'] = 1/samples_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mtype[1] == 'weighted':\n",
    "    dataloaders = {'train': DataLoader(dase_train, \n",
    "                                       batch_size=params['batch_size'], \n",
    "                                       num_workers=params['num_workers'], \n",
    "                                       shuffle=False,\n",
    "                                       sampler = params['sampler']),\n",
    "                  'test': DataLoader(dase_test, \n",
    "                                    batch_size=params['batch_size'], \n",
    "                                    num_workers=params['num_workers'], \n",
    "                                    shuffle=False)}\n",
    "elif mtype[1] == 'unweighted':\n",
    "    dataloaders = {'train': DataLoader(dase_train, \n",
    "                                   batch_size=params['batch_size'], \n",
    "                                   num_workers=params['num_workers'], \n",
    "                                   shuffle=True),\n",
    "                  'test': DataLoader(dase_test, \n",
    "                                    batch_size=params['batch_size'], \n",
    "                                    num_workers=params['num_workers'], \n",
    "                                    shuffle=False)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['num_feats'] = len(feana)\n",
    "\n",
    "if mtype[1] == 'weighted':\n",
    "    model = models.SDMNet(in_features=params['num_feats'], num_classes=params['num_classes'], \n",
    "                      num_filts=params['num_filts']).to(params['device'])\n",
    "elif mtype[1] == 'unweighted':\n",
    "    mod_file = whole_mod.replace('unweighted', 'weighted')\n",
    "    model=torch.load(mod_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mtype[2] == 'cel':\n",
    "     criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "elif mtype[2] == 'ndcg':\n",
    "    criterion = pytorchltr.loss.LambdaNDCGLoss1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define learning rates that change with epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up different learning rate\n",
    "if mtype[1] == 'weighted':\n",
    "    params['lr']['loc_lr'] = 0.5e-2\n",
    "    params['lr']['base_lr'] = 0.25e-2\n",
    "elif mtype[1] == 'unweighted':\n",
    "    params['lr']['loc_lr'] = 0.5e-2\n",
    "    params['lr']['base_lr'] = 0.25e-3  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define optimizer & scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD([{\"params\": model.feats.parameters(), \"lr\": params['lr']['loc_lr']},\n",
    "                             {\"params\": model.class_emb.parameters(), \"lr\": params['lr']['loc_lr']}],\n",
    "                             lr= params['lr']['base_lr'], momentum = 0.9)\n",
    "    \n",
    "# Define learing rate scheduler\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', threshold = 0.0005, factor=0.5, patience=15, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main model-fitting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to set up the training process\n",
    "def train_model(model, criterion, optimizer, params):\n",
    "    start = time.time()\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    num_epochs = params['num_epochs']\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for feats, labels in dataloaders[phase]: #feats,\n",
    "                feats = feats.to(params['device'])\n",
    "                labs = labels.to(params['device'])\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    outputs = model(feats) \n",
    "\n",
    "                    if mtype[2] == \"cel\":\n",
    "\n",
    "                        loss = criterion(outputs, labs) \n",
    "\n",
    "                    elif mtype[2] == \"ndcg\":\n",
    "\n",
    "                        lab_prob = torch.gather(outputs,1,labs.view(len(labs),1))\n",
    "\n",
    "                        mask = torch.ones_like(outputs).scatter_(1, labs.unsqueeze(1), 0.)\n",
    "                        zero_prob = outputs[mask.bool()].view(outputs.size()[0], (outputs.size()[1]-1))                      \n",
    "\n",
    "                        srted, indices = torch.sort(zero_prob, descending = True)\n",
    "                        preds = torch.cat((lab_prob.view(len(labs),1),srted[:,0:500]),1)\n",
    "\n",
    "                        n = torch.tensor(500).repeat(outputs.size()[0]).to(params['device'])\n",
    "\n",
    "                        relev = torch.zeros(outputs.size()[0], 501).to(params['device'])\n",
    "                        relev[:, 0] = 1\n",
    "\n",
    "                        loss = criterion(preds, relev, n).mean()                                  \n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()                       \n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * feats.size(0)\n",
    "                running_corrects += torch.sum(preds == labs)/dataset_sizes[phase]\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() \n",
    "\n",
    "            if phase == 'test':\n",
    "                    scheduler.step(epoch_loss)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "    time_el = time.time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_el // 60, time_el % 60))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(model, criterion, optimizer, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, whole_mod) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
